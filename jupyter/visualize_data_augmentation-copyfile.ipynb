{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7879f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from open3d.j_visualizer import JVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a391a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from torchsparse import SparseTensor\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "from torchsparse.utils.quantize import sparse_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43e5dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErasorCarlaInternal:\n",
    "\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 voxel_size,\n",
    "                 num_points,\n",
    "                 visualize,\n",
    "                 split,\n",
    "                 sample_stride=1,\n",
    "                 submit=False,\n",
    "                 google_mode=True,\n",
    "                 window=10,\n",
    "                 radius=50,\n",
    "                 save_npy=False,\n",
    "                 index=None):\n",
    "        if submit:\n",
    "            trainval = True\n",
    "        else:\n",
    "            trainval = False\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.voxel_size = voxel_size\n",
    "        self.num_points = num_points\n",
    "        self.sample_stride = sample_stride\n",
    "        self.google_mode = google_mode\n",
    "        self.window = window\n",
    "        self.visualize = visualize\n",
    "        self.save_npy = save_npy\n",
    "        self.index = index\n",
    "        self.radius = radius\n",
    "        self.seqs = []\n",
    "        if split == 'train':\n",
    "            self.seqs = [\n",
    "                'scenario2', 'scenario3', 'scenario5', 'scenario8',\n",
    "            ]\n",
    "\n",
    "        elif self.split == 'val':\n",
    "            self.seqs = [\n",
    "                'scenario6',\n",
    "            ]\n",
    "        elif self.split == 'test':\n",
    "            self.seqs = [\n",
    "               'scenario6',\n",
    "            ]\n",
    "\n",
    "        self.map_files = dict()\n",
    "        self.files = []\n",
    "\n",
    "        # get scan and map data list\n",
    "        for seq in self.seqs:\n",
    "            self.map_files[seq] = os.path.join(self.root, 'testing_map/v0.1', seq, 'map.npy')\n",
    "            seq_files = sorted(os.listdir(os.path.join(self.root, 'testing_data', seq, 'global_npz')))\n",
    "            seq_files = seq_files[(self.window + 1) // 2 - 1: - (self.window // 2) + 1]\n",
    "\n",
    "            seq_files = [os.path.join(self.root, 'testing_data', seq, 'global_npz', x) for x in seq_files]\n",
    "            self.files.extend(seq_files)\n",
    "\n",
    "        # get map_data\n",
    "        self.map = dict()\n",
    "        for seq, map_file in self.map_files.items():\n",
    "            map_ = np.load(map_file)\n",
    "            # map = generate_voxels(map_, voxel_size=self.voxel_size)\n",
    "            self.map[seq] = map_.astype(np.float32)\n",
    "\n",
    "        if self.sample_stride > 1:\n",
    "            self.files = self.files[::self.sample_stride]\n",
    "\n",
    "        self.num_classes = 2\n",
    "        self.angle = 0.0\n",
    "\n",
    "    def set_angle(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.index is not None:\n",
    "            index = self.index\n",
    "        \n",
    "        # get scan_data\n",
    "        scan = np.load(self.files[index])\n",
    "        block_ = scan['arr_0'].astype(np.float32)\n",
    "        odom = scan['arr_1'].astype(np.float32)\n",
    "        \n",
    "        # dst folder path\n",
    "        file_name = os.path.basename(self.files[index])\n",
    "        file_dir = os.path.dirname(self.files[index])\n",
    "        file_name_wo_ext = os.path.splitext(file_name)[0]  \n",
    "        folder_path = os.path.join(\"/ws/data/erasor_carla/carla_dataset/predictions/v\" \n",
    "                                       + str(self.voxel_size) \n",
    "                                       + \"_np\"\n",
    "                                       + str(self.num_points)\n",
    "                                       + \"_w\"\n",
    "                                       + str(self.window))\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "\n",
    "        folder_path2 = os.path.join(folder_path, file_name_wo_ext)\n",
    "        if not os.path.exists(folder_path2):\n",
    "                os.mkdir(folder_path2)\n",
    "\n",
    "                \n",
    "        if self.window > 1:\n",
    "            file_name = os.path.basename(self.files[index])\n",
    "            file_dir = os.path.dirname(self.files[index])\n",
    "            file_name_wo_ext = os.path.splitext(file_name)[0]\n",
    "            file_int = int(file_name_wo_ext)\n",
    "            file_int_list = list(range(file_int - (self.window + 1) // 2 + 1, file_int + self.window // 2 + 1, 1))\n",
    "            file_str_list = [file_dir + \"/\" + str(i).zfill(6) + \".npz\" for i in file_int_list]\n",
    "            block_ = []\n",
    "            block_multi = []\n",
    "            for file in file_str_list:\n",
    "                file_name = os.path.basename(file)\n",
    "                dst = os.path.join(folder_path2, file_name)\n",
    "                shutil.copy(file, dst)\n",
    "                print(\"file:\", file)\n",
    "                print(\"file_name: \", file_name)\n",
    "                print(\"folder_path2\", folder_path2)\n",
    "                scan_ = np.load(file)\n",
    "                block_single = scan_['arr_0'].astype(np.float32)\n",
    "                block_multi.append(block_single)\n",
    "#                 block_.extend(block_single)\n",
    "#             block_ = np.asarray(block_)\n",
    "#             # radius search w.r.t the odom of scan data\n",
    "#             block_ = block_[np.sum(np.square(block_[:, :3] - odom), axis=-1) < self.radius*self.radius]\n",
    "\n",
    "        # get map_data\n",
    "        scenario = self.files[index]\n",
    "        scenario = scenario.split(\"/\")[-3]\n",
    "        map_ = self.map[scenario]\n",
    "        # radius search w.r.t the odom of scan data\n",
    "        map_ = map_[np.sum(np.square(map_[:, :3] - odom), axis=-1) < self.radius*self.radius]\n",
    "        map_path = os.path.join(folder_path2, str(scenario) + \"_ori_map\" + file_name_wo_ext + \".npy\")\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "        np.save(map_path, map_)\n",
    "        \n",
    "        return map_, block_multi\n",
    "        \n",
    "#         if self.save_npy:\n",
    "#             folder_path = os.path.join(\"/ws/data/erasor_carla/carla_dataset/predictions/v\" \n",
    "#                                        + str(self.voxel_size) \n",
    "#                                        + \"_np\"\n",
    "#                                        + str(self.num_points)\n",
    "#                                        + \"_w\"\n",
    "#                                        + str(self.window))\n",
    "#             if not os.path.exists(folder_path):\n",
    "#                 os.mkdir(folder_path)\n",
    "            \n",
    "\n",
    "#             folder_path2 = os.path.join(folder_path, file_name_wo_ext)\n",
    "#             if not os.path.exists(folder_path2):\n",
    "#                 os.mkdir(folder_path2)\n",
    "#             map_path = os.path.join(folder_path2, str(scenario) + \"_ori_map\" + file_name_wo_ext + \".npy\")\n",
    "#             scan_path_list = [os.path.join(folder_path2, str(scenario) + \"_ori_scan\" + str(i).zfill(6) + \".npy\") for i in file_int_list]\n",
    "# #             scan_path = os.path.join(folder_path, str(scenario) + \"_ori_scan\" + file_name_wo_ext + \".npy\")\n",
    "#             odom_path = os.path.join(folder_path2, str(scenario) + \"_ori_odom\" + file_name_wo_ext + \".npy\")\n",
    "#             print(\"folder_path: \", folder_path)\n",
    "#             print(\"folder_path2: \", folder_path2)\n",
    "#             print(\"map_path: \", map_path)\n",
    "#             print(\"scan_path_list: \", scan_path_list)\n",
    "#             for i, scan_path in enumerate(scan_path_list):\n",
    "#                 np.save(scan_path, block_multi[i])\n",
    "    \n",
    "#             np.save(map_path, map_)\n",
    "#             np.save(odom_path, odom)\n",
    "        \n",
    "        \n",
    "#         # get pesudo label with ERASOR\n",
    "#         \"\"\"\n",
    "#         def get_pseudo_label(self, map_, block_multi, **kwargs):\n",
    "#             # return dynamic and static labels of map\n",
    "#             return map_[:, 3]\n",
    "#         pseudo_label = self.get_pseudo_label(map_, block_multi, **kwargs)\n",
    "#         \"\"\"\n",
    "        \n",
    "        \n",
    "#         block = np.zeros_like(block_)\n",
    "#         map = np.zeros_like(map_)\n",
    "        \n",
    "\n",
    "#         if 'train' in self.split:\n",
    "#             # data augmentation on the train dataset\n",
    "#             theta = np.random.uniform(0, 2 * np.pi)\n",
    "#             scale_factor = np.random.uniform(0.95, 1.05)\n",
    "#             rot_mat = np.array([[np.cos(theta), np.sin(theta), 0],\n",
    "#                                 [-np.sin(theta),\n",
    "#                                  np.cos(theta), 0], [0, 0, 1]])\n",
    "\n",
    "#             block[:, :3] = np.dot(block_[:, :3], rot_mat) * scale_factor\n",
    "#             map[:, :3] = np.dot(map_[:, :3], rot_mat) * scale_factor\n",
    "\n",
    "#         else:\n",
    "#             theta = self.angle\n",
    "#             transform_mat = np.array([[np.cos(theta),\n",
    "#                                        np.sif self.save_npy:\n",
    "#             folder_path = os.path.join(\"/ws/data/erasor_carla/carla_dataset/predictions/v\" \n",
    "#                                        + str(self.voxel_size) \n",
    "#                                        + \"_np\"\n",
    "#                                        + str(self.num_points)\n",
    "#                                        + \"_w\"\n",
    "#                                        + str(self.window))\n",
    "#             if not os.path.exists(folder_path):\n",
    "#                 os.mkdir(folder_path)\n",
    "            \n",
    "\n",
    "#             folder_path2 = os.path.join(folder_path, file_name_wo_ext)\n",
    "#             if not os.path.exists(folder_path2):\n",
    "#                 os.mkdir(folder_path2)\n",
    "#             map_path = os.path.join(folder_path2, str(scenario) + \"_ori_map\" + file_name_wo_ext + \".npy\")\n",
    "#             scan_path_list = [os.path.join(folder_path2, str(scenario) + \"_ori_scan\" + str(i).zfill(6) + \".npy\") for i in file_int_list]\n",
    "# #             scan_path = os.path.join(folder_path, str(scenario) + \"_ori_scan\" + file_name_wo_ext + \".npy\")\n",
    "#             odom_path = os.path.join(folder_path2, str(scenario) + \"_ori_odom\" + file_name_wo_ext + \".npy\")\n",
    "#             print(\"folder_path: \", folder_path)\n",
    "#             print(\"folder_path2: \", folder_path2)\n",
    "#             print(\"map_path: \", map_path)\n",
    "# #             print(\"scan_path_list: \", scan_path_list)\n",
    "#             for i, scan_path in enumerate(scan_path_list):\n",
    "#                 np.save(scan_path, block_multi[i])\n",
    "    \n",
    "#             np.save(map_path, map_)\n",
    "#             np.save(odom_path, odom)\n",
    "        \n",
    "        \n",
    "#         # get pesudo label with ERASOR\n",
    "#         \"\"\"\n",
    "#         def get_pseudo_label(self, map_, block_multi, **kwargs):\n",
    "#             # return dynamic and static labels of map\n",
    "#             return map_[:, 3]\n",
    "#         pseudo_label = self.get_pseudo_label(map_, block_multi, **kwargs)\n",
    "#         \"\"\"\n",
    "        \n",
    "        \n",
    "#         block = np.zeros_like(block_)\n",
    "#         map = np.zeros_like(map_)\n",
    "        \n",
    "\n",
    "#         if 'train' in self.split:\n",
    "#             # data augmentation on the train dataset\n",
    "#             theta = np.random.uniform(0, 2 * np.pi)\n",
    "#             scale_factor = np.random.uniform(0.95, 1.05)\n",
    "#             rot_mat = np.array([[np.cos(theta), np.sin(theta), 0],\n",
    "#                                 [-np.sin(theta),\n",
    "#                                  np.cos(theta), 0], [0, 0, 1]])\n",
    "\n",
    "#             block[:, :3] = np.dot(block_[:, :3], rot_mat) * scale_factor\n",
    "#             map[:, :3] = np.dot(map_[:, :3], rot_mat) * scale_factor\n",
    "\n",
    "#         else:\n",
    "#             theta = self.angle\n",
    "#             transform_mat = np.array([[np.cos(theta),\n",
    "#                                        np.sin(theta), 0],\n",
    "#                                       [-np.sin(theta),\n",
    "#                                        np.cos(theta), 0], [0, 0, 1]])\n",
    "#             block[...] = block_[...]\n",
    "#             block[:, :3] = np.dot(block[:, :3], transform_mat)\n",
    "#             map[:, :3] = np.dot(map_[:, :3], transform_mat)\n",
    "            \n",
    "# #         # self-supervised dynamic label\n",
    "# #         center_x = np.mean(block[:, 0])\n",
    "# #         center_y = np.mean(block[:, 1])\n",
    "# #         sspoints_ = np.mgrid[center_x+2:center_x+3:self.voxel_size, center_y-2:center_y+3:self.voxel_size, 0:2:self.voxel_size]\n",
    "# #         sspoints = sspoints_.reshape(3, -1).T\n",
    "# #         sslabels = np.ones([np.shape(sspoints)[0], 1],)\n",
    "# #         ssblock = np.concatenate((sspoints, sslabels), axis=1)\n",
    "        \n",
    "# #         block = np.concatenate((block, ssblock), axis=0)\n",
    "        \n",
    "\n",
    "#         # parsing the original label to the dynamic label\n",
    "#         block[:, 3] = (block[:, 3] == 1)\n",
    "#         map[:, 3] = (map[:, 3] == 1)\n",
    "        \n",
    "# #         # get point and voxel in the format of sparse torch tensor\n",
    "# #         map_data = self.get_point_voxel(map[:, :3], map[:, 3], index)\n",
    "# #         scan_data = self.get_point_voxel(block[:, :3], block[:, 3], index)\n",
    "        \n",
    "#         # get original point and voxel\n",
    "#         map_data = self.get_original_point(map[:, :3], map[:, 3], index)\n",
    "#         scan_data = self.get_original_point(block[:, :3], block[:, 3], index)\n",
    "\n",
    "# #         return map_data\n",
    "#         return map_data, scan_data\n",
    "# in(theta), 0],\n",
    "#                                       [-np.sin(theta),\n",
    "#                                        np.cos(theta), 0], [0, 0, 1]])\n",
    "#             block[...] = block_[...]\n",
    "#             block[:, :3] = np.dot(block[:, :3], transform_mat)\n",
    "#             map[:, :3] = np.dot(map_[:, :3], transform_mat)\n",
    "            \n",
    "# #         # self-supervised dynamic label\n",
    "# #         center_x = np.mean(block[:, 0])\n",
    "# #         center_y = np.mean(block[:, 1])\n",
    "# #         sspoints_ = np.mgrid[center_x+2:center_x+3:self.voxel_size, center_y-2:center_y+3:self.voxel_size, 0:2:self.voxel_size]\n",
    "# #         sspoints = sspoints_.reshape(3, -1).T\n",
    "# #         sslabels = np.ones([np.shape(sspoints)[0], 1],)\n",
    "# #         ssblock = np.concatenate((sspoints, sslabels), axis=1)\n",
    "        \n",
    "# #         block = np.concatenate((block, ssblock), axis=0)\n",
    "        \n",
    "\n",
    "#         # parsing the original label to the dynamic label\n",
    "#         block[:, 3] = (block[:, 3] == 1)\n",
    "#         map[:, 3] = (map[:, 3] == 1)\n",
    "        \n",
    "# #         # get point and voxel in the format of sparse torch tensor\n",
    "# #         map_data = self.get_point_voxel(map[:, :3], map[:, 3], index)\n",
    "# #         scan_data = self.get_point_voxel(block[:, :3], block[:, 3], index)\n",
    "        \n",
    "#         # get original point and voxel\n",
    "#         map_data = self.get_original_point(map[:, :3], map[:, 3], index)\n",
    "#         scan_data = self.get_original_point(block[:, :3], block[:, 3], index)\n",
    "\n",
    "# #         return map_data\n",
    "#         return map_data, scan_data\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(inputs):\n",
    "        return sparse_collate_fn(inputs)\n",
    "    \n",
    "    def get_original_point(self, block, labels_, index):\n",
    "        pc_ = np.round(block[:, :3] / self.voxel_size).astype(np.int32)\n",
    "        \n",
    "#         # normalize the point for visualization\n",
    "#         pc_ = (pc_ - np.mean(pc_)) / np.std(pc_)\n",
    "        pc_ -= pc_.min(0, keepdims=1)\n",
    "        \n",
    "        _, inds, inverse_map = sparse_quantize(pc_,\n",
    "                                               return_index=True,\n",
    "                                               return_inverse=True)\n",
    "        \n",
    "        if len(inds) > self.num_points:\n",
    "                inds = np.random.choice(inds, self.num_points, replace=False)\n",
    "    \n",
    "        pc = pc_[inds]\n",
    "        labels = labels_[inds]\n",
    "        \n",
    "        \n",
    "        feed_dict = {\n",
    "            'pc_': pc_,\n",
    "            'targets_': labels_,\n",
    "            'file_name': self.files[index],\n",
    "            'pc': pc,\n",
    "            'targets': labels,\n",
    "            'inds': inds,\n",
    "            'inverse_map': inverse_map,\n",
    "            'file_name': self.files[index]\n",
    "        }\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_point_voxel(self, block, labels_, index):\n",
    "        pc_ = np.round(block[:, :3] / self.voxel_size).astype(np.int32)\n",
    "        pc_ -= pc_.min(0, keepdims=1)\n",
    "\n",
    "        feat_ = block\n",
    "\n",
    "        _, inds, inverse_map = sparse_quantize(pc_,\n",
    "                                               return_index=True,\n",
    "                                               return_inverse=True)\n",
    "\n",
    "        if 'train' in self.split:\n",
    "            if len(inds) > self.num_points:\n",
    "                inds = np.random.choice(inds, self.num_points, replace=False)\n",
    "\n",
    "        pc = pc_[inds]\n",
    "        feat = feat_[inds]\n",
    "        labels = labels_[inds]\n",
    "        lidar = SparseTensor(feat, pc)\n",
    "        labels = SparseTensor(labels, pc)\n",
    "        labels_ = SparseTensor(labels_, pc_)\n",
    "        inverse_map = SparseTensor(inverse_map, pc_)\n",
    "\n",
    "        if self.visualize == False:\n",
    "            feed_dict = {\n",
    "            'lidar': lidar,\n",
    "            'targets': labels,\n",
    "            'targets_mapped': labels_,\n",
    "            'inverse_map': inverse_map,\n",
    "            'file_name': self.files[index]\n",
    "        }\n",
    "\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                'pc': block[:, :3],\n",
    "                'label': labels_,\n",
    "                'lidar': lidar,\n",
    "                'targets': labels,\n",
    "                'targets_mapped': labels_,\n",
    "                'inverse_map': inverse_map,\n",
    "                'file_name': self.files[index]\n",
    "            }\n",
    "\n",
    "        return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa00e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/ws/data/erasor_carla/carla_dataset\"\n",
    "voxel_size = 0.1\n",
    "num_points = 1000000\n",
    "visualize = True\n",
    "sample_stride = 1\n",
    "split = \"train\"\n",
    "submit = True\n",
    "window = 10\n",
    "save_npy = True\n",
    "index = 330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15d641ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ErasorCarlaInternal(root, voxel_size, num_points, visualize, split, submit, save_npy=save_npy, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70131fe2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000057.npz\n",
      "file_name:  000057.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000058.npz\n",
      "file_name:  000058.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000059.npz\n",
      "file_name:  000059.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000060.npz\n",
      "file_name:  000060.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000061.npz\n",
      "file_name:  000061.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000062.npz\n",
      "file_name:  000062.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000063.npz\n",
      "file_name:  000063.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000064.npz\n",
      "file_name:  000064.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000065.npz\n",
      "file_name:  000065.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n",
      "file: /ws/data/erasor_carla/carla_dataset/testing_data/scenario3/global_npz/000066.npz\n",
      "file_name:  000066.npz\n",
      "folder_path2 /ws/data/erasor_carla/carla_dataset/predictions/v0.1_np1000000_w10/000061\n"
     ]
    }
   ],
   "source": [
    "map_data, scan_data = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d52fc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127676, 4)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55afe60b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1169798, 4)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e43b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e98bd07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67301, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(map_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0333f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5050426e+01, -2.1462938e+02,  5.8639407e+00,  0.0000000e+00],\n",
       "       [-2.4233816e+01, -2.2499368e+02,  6.2504468e+00,  0.0000000e+00],\n",
       "       [-2.4581617e+01, -2.2499365e+02,  6.2520766e+00,  0.0000000e+00],\n",
       "       ...,\n",
       "       [-9.5500336e+00, -1.1970595e+02,  3.6437996e-08,  0.0000000e+00],\n",
       "       [-9.5660181e+00, -1.1970617e+02, -6.8335794e-08,  0.0000000e+00],\n",
       "       [-9.5820341e+00, -1.1970628e+02, -1.3143290e-07,  0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "252a1787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126756, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db40e27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pc_', 'targets_', 'file_name', 'pc', 'targets', 'inds', 'inverse_map'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scan_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c313907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(scan_data['pc_'])\n",
    "# np.shape(scan_data['pc'][scan_data['inverse_map']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af296dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ = scan_data['pc']\n",
    "labels = scan_data['targets']\n",
    "orig_points = scan_data['pc'][scan_data['inverse_map']]\n",
    "orig_labels = scan_data['targets'][scan_data['inverse_map']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40573949",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = (points_ - points_.mean()) / points_.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a82300",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2color = {0: (1.0, 0.0, 0.0), 1: (0.0, 0.0, 1.0), 2:(0.0, 0.0, 1.0)}\n",
    "colors = np.array([label2color[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "777061fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f4a4f5dd2141e58b8f5ae37e093c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3bbb631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231053, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "608d7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpoints_ = map_data['pc']\n",
    "mlabels = map_data['targets']\n",
    "# orig_points = map_data['pc'][map_data['inverse_map']]\n",
    "# orig_labels = map_data['targets'][map_data['inverse_map']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f312cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpoints = (mpoints_ - mpoints_.mean()) / mpoints_.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30268d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2color = {0: (1.0, 0.0, 0.0), 1: (0.0, 0.0, 1.0), 2:(0.0, 0.0, 1.0)}\n",
    "mcolors = np.array([label2color[x] for x in mlabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4207e03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd02115aa1f4f0996ac432422179ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(mpoints)\n",
    "pcd.colors = o3d.utility.Vector3dVector(mcolors)\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60766546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(mpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03fa1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sspoints_ = np.mgrid[1:3:0.2, 2:6:0.2, 0:2:0.2]\n",
    "sspoints = sspoints_.reshape(3, -1).T\n",
    "sslabels = np.ones([np.shape(sspoints)[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dd33be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19546345,  0.59915196, -1.19161234],\n",
       "       [-1.18391013,  0.59530085, -1.19161234],\n",
       "       [-1.17235681,  0.58759864, -1.19161234],\n",
       "       ...,\n",
       "       [ 1.97399681, -0.34822012, -1.01831257],\n",
       "       [ 1.99710345, -0.29045353, -1.01831257],\n",
       "       [ 1.99710345, -0.28660243, -1.01831257]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "79e82247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sspoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "04a7b2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sslabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7ceeafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssblock = np.concatenate((sspoints, sslabels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bd40667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.concatenate((points, sspoints), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c7d2c991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.18889817,  0.44287013, -1.18523127],\n",
       "       [-1.18889817,  0.45387082, -1.18523127],\n",
       "       [-1.18523127,  0.39886739, -1.18523127],\n",
       "       ...,\n",
       "       [ 5.8       ,  4.8       ,  1.4       ],\n",
       "       [ 5.8       ,  4.8       ,  1.6       ],\n",
       "       [ 5.8       ,  4.8       ,  1.8       ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1d347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15072fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12b6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb83b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507bf9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
