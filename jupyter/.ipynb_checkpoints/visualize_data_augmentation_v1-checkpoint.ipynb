{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7879f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from open3d.j_visualizer import JVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a391a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "from torchsparse import SparseTensor\n",
    "from torchsparse.utils.collate import sparse_collate_fn\n",
    "from torchsparse.utils.quantize import sparse_quantize\n",
    "\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43e5dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErasorCarlaInternal:\n",
    "\n",
    "    def __init__(self,\n",
    "                 root,\n",
    "                 voxel_size,\n",
    "                 num_points,\n",
    "                 visualize,\n",
    "                 split,\n",
    "                 proposal=False,\n",
    "                 sample_stride=1,\n",
    "                 submit=False,\n",
    "                 google_mode=True,\n",
    "                 window=10,\n",
    "                 radius=50,\n",
    "                 thres=0.02,\n",
    "                 save_npy=False,\n",
    "                 index=None):\n",
    "        \n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.proposal = proposal\n",
    "        self.submit = submit\n",
    "        self.google_mode = google_mode\n",
    "        self.visualize = visualize\n",
    "        self.save_npy = save_npy # only in jupyter\n",
    "        self.index = index # only in jupyter\n",
    "\n",
    "        self.voxel_size = voxel_size\n",
    "        self.num_points = num_points\n",
    "        self.sample_stride = sample_stride\n",
    "        self.window = window\n",
    "        self.radius = radius\n",
    "        self.thres = thres\n",
    "        \n",
    "        self.seqs = []\n",
    "        if split == 'train':\n",
    "            if self.proposal == 'erasor':\n",
    "                print(\"self.proposal: \", self.proposal)\n",
    "                self.seqs = [\n",
    "                    'scenario3', 'scenario5', 'scenario8',\n",
    "                ]\n",
    "                print(\"scenario: \", self.seqs)\n",
    "            else:\n",
    "                print(\"self.proposal: \", self.proposal)\n",
    "                self.seqs = [\n",
    "                    'scenario2', 'scenario3', 'scenario5', 'scenario8',\n",
    "\n",
    "                ]\n",
    "                print(\"scenario: \", self.seqs)\n",
    "\n",
    "            \n",
    "            if self.submit:\n",
    "                self.seqs.append('scenario6')\n",
    "\n",
    "        elif self.split == 'val':\n",
    "            self.seqs = [\n",
    "                'scenario6',\n",
    "            ]\n",
    "            print(\"scenario: \", self.seqs)\n",
    "\n",
    "        elif self.split == 'test':\n",
    "            self.seqs = [\n",
    "               'scenario6',\n",
    "            ]\n",
    "\n",
    "        self.map_files = dict()\n",
    "        self.scan_files = []\n",
    "\n",
    "        # get scan and map data list\n",
    "        for seq in self.seqs:\n",
    "            map_type = 'testing_map/v0.1_erasor_patchwork' if self.proposal=='erasor' else 'testing_map/v0.1'\n",
    "            print(\"map_type: \", map_type)\n",
    "            self.map_files[seq] = os.path.join(self.root, map_type, seq, 'map.npy')\n",
    "            seq_files = sorted(os.listdir(os.path.join(self.root, 'testing_data', seq, 'global_npz')))\n",
    "            seq_files = seq_files[(self.window + 1) // 2 - 1: - (1 + self.window // 2) ]\n",
    "\n",
    "            seq_files = [os.path.join(self.root, 'testing_data', seq, 'global_npz', x) for x in seq_files]\n",
    "            self.scan_files.extend(seq_files)\n",
    "\n",
    "        # get map_data\n",
    "        self.map = dict()\n",
    "        for seq, map_file in self.map_files.items():\n",
    "            map_ = np.load(map_file)\n",
    "            # get pesudo label with ERASOR\n",
    "            \"\"\"\n",
    "            def get_pseudo_label(self, map_, block_multi, **kwargs):\n",
    "                # return dynamic and static labels of map\n",
    "                return map_[:, 3]\n",
    "            pseudo_label = self.get_pseudo_label(map_, block_multi, **kwargs)\n",
    "            \"\"\"\n",
    "            # map = generate_voxels(map_, voxel_size=self.voxel_size)\n",
    "#             map_[:, 3] = self.revise_dynamic_points(map_[:, :3], map_[:, 3], self.thres)\n",
    "            self.map[seq] = map_.astype(np.float32)\n",
    "\n",
    "        if self.sample_stride > 1:\n",
    "            self.scan_files = self.scan_files[::self.sample_stride]\n",
    "    \n",
    "\n",
    "        self.num_classes = 2\n",
    "        self.angle = 0.0\n",
    "    \n",
    "    def revise_dynamic_points(self, points, labels, thres):\n",
    "        labels[(labels != 0) & (points[:, 2] < thres)] = 0\n",
    "        return labels\n",
    "    \n",
    "    def concatenate_scans(self):\n",
    "        file_name = os.path.basename(self.scan_files[index])\n",
    "        file_dir = os.path.dirname(self.scan_files[index])\n",
    "        file_name_wo_ext = os.path.splitext(file_name)[0]\n",
    "        file_int = int(file_name_wo_ext)\n",
    "        file_int_list = list(range(file_int - (self.window + 1) // 2 + 1, file_int + self.window // 2 + 1, 1))\n",
    "        file_str_list = [file_dir + \"/\" + str(i).zfill(6) + \".npz\" for i in file_int_list]\n",
    "        block_ = []\n",
    "        for file in file_str_list:\n",
    "            scan_ = np.load(file)\n",
    "            block_single = scan_['arr_0'].astype(np.float32)\n",
    "            block_.extend(block_single)\n",
    "        block_ = np.asarray(block_)\n",
    "        # radius search w.r.t the odom of scan data\n",
    "        block_ = block_[np.sum(np.square(block_[:, :3] - odom), axis=-1) < self.radius*self.radius]\n",
    "        \n",
    "        return block_\n",
    "    \n",
    "    def save_npy(self, map, odom):\n",
    "        folder_path = os.path.join(\"/ws/data/erasor_carla/carla_dataset/debug/v\" \n",
    "                                       + str(self.voxel_size) \n",
    "                                       + \"_np\"\n",
    "                                       + str(self.num_points)\n",
    "                                       + \"_w\"\n",
    "                                       + str(self.window))\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "    \n",
    "        file_name = os.path.basename(self.scan_files[index])\n",
    "        file_dir = os.path.dirname(self.scan_files[index])\n",
    "        file_name_wo_ext = os.path.splitext(file_name)[0]\n",
    "\n",
    "        folder_path2 = os.path.join(folder_path, file_name_wo_ext)\n",
    "        if not os.path.exists(folder_path2):\n",
    "            os.mkdir(folder_path2)\n",
    "        map_path = os.path.join(folder_path2, str(scenario) + \"_ori_map\" + file_name_wo_ext + \".npy\")\n",
    "        scan_path_list = [os.path.join(folder_path2, str(scenario) + \"_ori_scan\" + str(i).zfill(6) + \".npy\") for i in file_int_list]\n",
    "        odom_path = os.path.join(folder_path2, str(scenario) + \"_ori_odom\" + file_name_wo_ext + \".npy\")\n",
    "\n",
    "        np.save(map_path, map)\n",
    "        np.save(odom_path, odom)\n",
    "\n",
    "    def set_angle(self, angle):\n",
    "        self.angle = angle\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scan_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        if self.index is not None:\n",
    "            index = self.index\n",
    "        \n",
    "        # get scan_data\n",
    "        scan = np.load(self.scan_files[index])\n",
    "        block_ = scan['arr_0'].astype(np.float32)\n",
    "        odom = scan['arr_1'].astype(np.float32)\n",
    "        print(\"original scan point max label: \", block_[:, 3].max())\n",
    "        \n",
    "#         # concatenate the scan data with window size\n",
    "#         if self.window > 1:\n",
    "#             block_ = self.concatenate_scans()\n",
    "        \n",
    "        if self.window > 1:\n",
    "            file_name = os.path.basename(self.scan_files[index])\n",
    "            file_dir = os.path.dirname(self.scan_files[index])\n",
    "            file_name_wo_ext = os.path.splitext(file_name)[0]\n",
    "            file_int = int(file_name_wo_ext)\n",
    "            file_int_list = list(range(file_int - (self.window + 1) // 2 + 1, file_int + self.window // 2 + 1, 1))\n",
    "            file_str_list = [file_dir + \"/\" + str(i).zfill(6) + \".npz\" for i in file_int_list]\n",
    "            block_ = []\n",
    "            for file in file_str_list:\n",
    "                scan_ = np.load(file)\n",
    "                block_single = scan_['arr_0'].astype(np.float32)\n",
    "                block_.extend(block_single)\n",
    "            block_ = np.asarray(block_)\n",
    "            # radius search w.r.t the odom of scan data\n",
    "            block_ = block_[np.sum(np.square(block_[:, :3] - odom), axis=-1) < self.radius*self.radius]\n",
    "        print(\"scan point max label: \", block_[:, 3].max())\n",
    "        \n",
    "        # get map_data\n",
    "        scenario = self.scan_files[index]\n",
    "        scenario = scenario.split(\"/\")[-3]\n",
    "        map_ = self.map[scenario]\n",
    "        map_label = np.array(map_[:, 3])\n",
    "        print(\"original map point max label: \", map_[:, 3].max())\n",
    "        print(\"map_label: \", map_label)\n",
    "        print(\"orignal; map point len: \", np.shape(map_label))\n",
    "        print(\"orignal; map point 0 len: \", np.shape(map_label[map_label==0]))\n",
    "        print(\"orignal; map point 1 len: \", np.shape(map_label[map_label==1]))\n",
    "        print(\"orignal; map point 2 len: \", np.shape(map_label[map_label==2]))\n",
    "        print(\"orignal; map point not 0 len: \", np.shape(map_label[map_label!=0]))\n",
    "        print(\"orignal; map point 700 len: \", np.shape(map_label[map_label==700]))\n",
    "            \n",
    "        # radius search w.r.t the odom of scan data\n",
    "        map_ = map_[np.sum(np.square(map_[:, :3] - odom), axis=-1) < self.radius*self.radius]\n",
    "        print(\"radius map point max label: \", map_[:, 3].max())\n",
    "                        \n",
    "        # save npy\n",
    "        if self.save_npy:\n",
    "            self.save_npy(map_, odom)\n",
    "#             folder_path = os.path.join(\"/ws/data/erasor_carla/carla_dataset/debug/v\" \n",
    "#                                        + str(self.voxel_size) \n",
    "#                                        + \"_np\"\n",
    "#                                        + str(self.num_points)\n",
    "#                                        + \"_w\"\n",
    "#                                        + str(self.window))\n",
    "#             if not os.path.exists(folder_path):\n",
    "#                 os.mkdir(folder_path)\n",
    "            \n",
    "\n",
    "#             folder_path2 = os.path.join(folder_path, file_name_wo_ext)\n",
    "#             if not os.path.exists(folder_path2):\n",
    "#                 os.mkdir(folder_path2)\n",
    "#             map_path = os.path.join(folder_path2, str(scenario) + \"_ori_map\" + file_name_wo_ext + \".npy\")\n",
    "#             scan_path_list = [os.path.join(folder_path2, str(scenario) + \"_ori_scan\" + str(i).zfill(6) + \".npy\") for i in file_int_list]\n",
    "# #             scan_path = os.path.join(folder_path, str(scenario) + \"_ori_scan\" + file_name_wo_ext + \".npy\")\n",
    "#             odom_path = os.path.join(folder_path2, str(scenario) + \"_ori_odom\" + file_name_wo_ext + \".npy\")\n",
    "#             print(\"folder_path: \", folder_path)\n",
    "#             print(\"folder_path2: \", folder_path2)\n",
    "#             print(\"map_path: \", map_path)\n",
    "# #             print(\"scan_path_list: \", scan_path_list)\n",
    "# #             for i, scan_path in enumerate(scan_path_list):\n",
    "# #                 np.save(scan_path, block_multi[i])\n",
    "    \n",
    "#             np.save(map_path, map_)\n",
    "#             np.save(odom_path, odom)\n",
    "                \n",
    "        \n",
    "        block = np.zeros_like(block_)\n",
    "        map = np.zeros_like(map_)\n",
    "        \n",
    "\n",
    "        if 'train' in self.split:\n",
    "            # data augmentation on the train dataset: off in the ipynb\n",
    "#             theta = np.random.uniform(0, 2 * np.pi)\n",
    "#             scale_factor = np.random.uniform(0.95, 1.05)\n",
    "#             rot_mat = np.array([[np.cos(theta), np.sin(theta), 0],\n",
    "#                                 [-np.sin(theta),\n",
    "#                                  np.cos(theta), 0], [0, 0, 1]])\n",
    "\n",
    "#             block[:, :3] = np.dot(block_[:, :3], rot_mat) * scale_factor\n",
    "#             map[:, :3] = np.dot(map_[:, :3], rot_mat) * scale_factor\n",
    "            theta = self.angle\n",
    "            transform_mat = np.array([[np.cos(theta),\n",
    "                                       np.sin(theta), 0],\n",
    "                                      [-np.sin(theta),\n",
    "                                       np.cos(theta), 0], [0, 0, 1]])\n",
    "            block[...] = block_[...]\n",
    "            map[:, :3] = map_[:, :3]\n",
    "\n",
    "        else:\n",
    "            theta = self.angle\n",
    "            transform_mat = np.array([[np.cos(theta),\n",
    "                                       np.sin(theta), 0],\n",
    "                                      [-np.sin(theta),\n",
    "                                       np.cos(theta), 0], [0, 0, 1]])\n",
    "            block[...] = block_[...]\n",
    "#             block[:, :3] = np.dot(block[:, :3], transform_mat)\n",
    "#             map[:, :3] = np.dot(map_[:, :3], transform_mat)\n",
    "            map[:, :3] = map_[:, :3]\n",
    "\n",
    "        \n",
    "#         # self-supervised dynamic label\n",
    "#         center_x = np.mean(block[:, 0])\n",
    "#         center_y = np.mean(block[:, 1])\n",
    "#         sspoints_ = np.mgrid[center_x+2:center_x+3:self.voxel_size, center_y-2:center_y+3:self.voxel_size, 0:2:self.voxel_size]\n",
    "#         sspoints = sspoints_.reshape(3, -1).T\n",
    "#         sslabels = np.ones([np.shape(sspoints)[0], 1],)\n",
    "#         ssblock = np.concatenate((sspoints, sslabels), axis=1)\n",
    "        \n",
    "#         block = np.concatenate((block, ssblock), axis=0)\n",
    "        \n",
    "\n",
    "        # parsing the original label to the dynamic label\n",
    "#         block[:, 3] = (block_[:, 3] == 1)\n",
    "#         dynamic = 700 if self.proposal == 'erasor' else 1\n",
    "#         map[:, 3] = (map_[:, 3] == dynamic)\n",
    "#         map[:, 3] = (map_[:, 3] != 0)\n",
    "        block[:, 3] = (block_[:, 3] != 0)\n",
    "        map[:, 3] = (map_[:, 3] != 0 )\n",
    "        print(\"parsed scan max point: \", block[: , 3].max())\n",
    "        print(\"parsed scan min point: \", block[:, 3].min())\n",
    "        print(\"parsed map max point: \", map[:, 3].max())\n",
    "        print(\"parsed map min point: \", map[:, 3].min())\n",
    "        \n",
    "#         # get point and voxel in the format of sparse torch tensor\n",
    "#         map_data = self.get_point_voxel(map[:, :3], map[:, 3], index)\n",
    "#         scan_data = self.get_point_voxel(block[:, :3], block[:, 3], index)\n",
    "        \n",
    "        # get original point and voxel\n",
    "        map_data = self.get_original_point(map[:, :3], map[:, 3], index)\n",
    "        scan_data = self.get_original_point(block[:, :3], block[:, 3], index)\n",
    "        \n",
    "#         # Enlarge ERASOR points\n",
    "#         map_data['targets'] = self.propose_near_dynamic_points(map_data['pc'], map_data['targets'])\n",
    "        \n",
    "        return map_data, scan_data\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(inputs):\n",
    "        return sparse_collate_fn(inputs)\n",
    "    \n",
    "    def propose_near_dynamic_points(self, points, labels, leaf_size=40, k=10):\n",
    "        \"\"\"propose top k near points from dynamic points\"\"\"\n",
    "        # nearest point search intialization\n",
    "        tree = KDTree(points, leaf_size=leaf_size) \n",
    "        dist, ind = tree.query(points, k=k) \n",
    "        labels[np.unique(ind[labels==1])] = 1 #  index of the top 5 near points from dynamic points -> set -> label the point as dynamic point\n",
    "        \n",
    "        return labels\n",
    "    \n",
    "    def get_original_point(self, block, labels_, index):\n",
    "        pc_ = np.round(block[:, :3] / self.voxel_size).astype(np.int32)\n",
    "        \n",
    "#         # normalize the point for visualization\n",
    "#         pc_ = (pc_ - np.mean(pc_)) / np.std(pc_)\n",
    "        pc_ -= pc_.min(0, keepdims=1)\n",
    "        \n",
    "        _, inds, inverse_map = sparse_quantize(pc_,\n",
    "                                               return_index=True,\n",
    "                                               return_inverse=True)\n",
    "        \n",
    "        if len(inds) > self.num_points:\n",
    "                inds = np.random.choice(inds, self.num_points, replace=False)\n",
    "    \n",
    "        pc = pc_[inds]\n",
    "        labels = labels_[inds]\n",
    "        \n",
    "        \n",
    "        feed_dict = {\n",
    "            'pc_': pc_,\n",
    "            'block_': block,\n",
    "            'targets_': labels_,\n",
    "            'file_name': self.scan_files[index],\n",
    "            'pc': pc,\n",
    "            'targets': labels,\n",
    "            'inds': inds,\n",
    "            'inverse_map': inverse_map,\n",
    "            'file_name': self.scan_files[index]\n",
    "        }\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_point_voxel(self, block, labels_, index):\n",
    "        pc_ = np.round(block[:, :3] / self.voxel_size).astype(np.int32)\n",
    "        pc_ -= pc_.min(0, keepdims=1)\n",
    "\n",
    "        feat_ = block\n",
    "\n",
    "        _, inds, inverse_map = sparse_quantize(pc_,\n",
    "                                               return_index=True,\n",
    "                                               return_inverse=True)\n",
    "\n",
    "        if 'train' in self.split:\n",
    "            if len(inds) > self.num_points:\n",
    "                inds = np.random.choice(inds, self.num_points, replace=False)\n",
    "\n",
    "        pc = pc_[inds]\n",
    "        feat = feat_[inds]\n",
    "        labels = labels_[inds]\n",
    "        lidar = SparseTensor(feat, pc)\n",
    "        labels = SparseTensor(labels, pc)\n",
    "        labels_ = SparseTensor(labels_, pc_)\n",
    "        inverse_map = SparseTensor(inverse_map, pc_)\n",
    "\n",
    "        if self.visualize == False:\n",
    "            feed_dict = {\n",
    "            'lidar': lidar,\n",
    "            'targets': labels,\n",
    "            'targets_mapped': labels_,\n",
    "            'inverse_map': inverse_map,\n",
    "            'file_name': self.scan_files[index]\n",
    "        }\n",
    "\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                'pc': block[:, :3],\n",
    "                'label': labels_,\n",
    "                'lidar': lidar,\n",
    "                'targets': labels,\n",
    "                'targets_mapped': labels_,\n",
    "                'inverse_map': inverse_map,\n",
    "                'file_name': self.scan_files[index]\n",
    "            }\n",
    "\n",
    "        return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa00e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/ws/data/erasor_carla/carla_dataset\"\n",
    "voxel_size = 0.1\n",
    "num_points = 180000\n",
    "visualize = True\n",
    "sample_stride = 1\n",
    "split = \"train\"\n",
    "proposal = None\n",
    "submit = True\n",
    "window = 10\n",
    "save_npy = False\n",
    "index = 370\n",
    "radius = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15d641ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.proposal:  None\n",
      "scenario:  ['scenario2', 'scenario3', 'scenario5', 'scenario8']\n",
      "map_type:  testing_map/v0.1\n",
      "map_type:  testing_map/v0.1\n",
      "map_type:  testing_map/v0.1\n",
      "map_type:  testing_map/v0.1\n"
     ]
    }
   ],
   "source": [
    "dataset = ErasorCarlaInternal(root, voxel_size, num_points, visualize, split, proposal, submit, window=window, radius=radius, index=index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70131fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original scan point max label:  141.0\n",
      "scan point max label:  141.0\n",
      "original map point max label:  141.0\n",
      "map_label:  [0. 0. 0. ... 0. 0. 0.]\n",
      "orignal; map point len:  (5176474,)\n",
      "orignal; map point 0 len:  (4979920,)\n",
      "orignal; map point 1 len:  (178051,)\n",
      "orignal; map point 2 len:  (0,)\n",
      "orignal; map point not 0 len:  (196554,)\n",
      "orignal; map point 700 len:  (0,)\n",
      "radius map point max label:  141.0\n",
      "parsed scan max point:  1.0\n",
      "parsed scan min point:  0.0\n",
      "parsed map max point:  1.0\n",
      "parsed map min point:  0.0\n"
     ]
    }
   ],
   "source": [
    "map_data, scan_data = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88ca4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "643a027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpoints = map_data['pc']\n",
    "mlabels = map_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4d4867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = scan_data['pc']\n",
    "labels = scan_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "889c3064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_points(points, labels, label2color):\n",
    "    points = (points - points.mean()) / points.std()\n",
    "    colors = np.array([label2color[x] for x in labels])\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    visualizer = JVisualizer()\n",
    "    visualizer.add_geometry(pcd)\n",
    "    visualizer.show()\n",
    "\n",
    "def save_points(points, labels, label2color, path):\n",
    "    points = (points - points.mean()) / points.std()\n",
    "    colors = np.array([label2color[x] for x in labels])\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.io.write_point_cloud(path, pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a08d8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2color = {0: (1.0, 0.0, 0.0), 1: (0.0, 0.0, 1.0), 2:(0.0, 0.0, 1.0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c90a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_points(points, labels, label2color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "617d4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_points(mpoints, mlabels, label2color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c64904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/ws/data/erasor_carla/carla_dataset/figure/v0.2/map_index{}_w10_gt.pcd\".format(index)\n",
    "save_points(mpoints, mlabels, label2color, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "ab9ea7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tree = KDTree(mpoints, leaf_size=40) # nearest point search intialization\n",
    "dist, ind = tree.query(mpoints, k=5) # search top 5 near points and return distance and index\n",
    "mlabels[np.unique(ind[mlabels==1])] = 1 #  index of the top 5 near points from dynamic points -> set -> label the point as dynamic point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e1457",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_points(mpoints, mlabels, label2color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29accfc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "c660e1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26c37f4186e4a69b2cdfa9f146d27e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_points(mpoints, mlabels, label2color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "af296dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ = scan_data['pc']\n",
    "labels = scan_data['targets']\n",
    "orig_points = scan_data['pc'][scan_data['inverse_map']]\n",
    "orig_labels = scan_data['targets'][scan_data['inverse_map']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "40573949",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = (points_ - points_.mean()) / points_.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b9522a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183949,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d8a82300",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2color = {0: (1.0, 0.0, 0.0), 1: (0.0, 0.0, 1.0), 2:(0.0, 0.0, 1.0)}\n",
    "colors = np.array([label2color[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "777061fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d0ca598598489ca95631646709bc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points)\n",
    "pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e3bbb631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183949, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "608d7a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpoints_ = map_data['pc']\n",
    "mlabels = map_data['targets']\n",
    "# orig_points = map_data['pc'][map_data['inverse_map']]\n",
    "# orig_labels = map_data['targets'][map_data['inverse_map']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f312cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpoints = (mpoints_ - mpoints_.mean()) / mpoints_.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "30268d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2color = {0: (1.0, 0.0, 0.0), 1: (0.0, 0.0, 1.0), 2:(0.0, 0.0, 1.0)}\n",
    "mcolors = np.array([label2color[x] for x in mlabels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b4207e03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74621094aae048419b8289b95dfe91fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(mpoints)\n",
    "pcd.colors = o3d.utility.Vector3dVector(mcolors)\n",
    "visualizer = JVisualizer()\n",
    "visualizer.add_geometry(pcd)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78183011",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = KDTree(mpoints, leaf_size=40) # nearest point search intialization\n",
    "dist, ind = tree.query(mpoints, k=5) # search top 5 near points and return distance and index\n",
    "mlabels[np.unique(ind[mlabels==1])] = 1 #  index of the top 5 near points from dynamic points -> set -> label the point as dynamic point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ca7f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f47c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "60766546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240000, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(mpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "03fa1eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sspoints_ = np.mgrid[1:3:0.2, 2:6:0.2, 0:2:0.2]\n",
    "sspoints = sspoints_.reshape(3, -1).T\n",
    "sslabels = np.ones([np.shape(sspoints)[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dd33be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.19546345,  0.59915196, -1.19161234],\n",
       "       [-1.18391013,  0.59530085, -1.19161234],\n",
       "       [-1.17235681,  0.58759864, -1.19161234],\n",
       "       ...,\n",
       "       [ 1.97399681, -0.34822012, -1.01831257],\n",
       "       [ 1.99710345, -0.29045353, -1.01831257],\n",
       "       [ 1.99710345, -0.28660243, -1.01831257]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "79e82247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sspoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "04a7b2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sslabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "7ceeafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssblock = np.concatenate((sspoints, sslabels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bd40667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.concatenate((points, sspoints), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c7d2c991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.18889817,  0.44287013, -1.18523127],\n",
       "       [-1.18889817,  0.45387082, -1.18523127],\n",
       "       [-1.18523127,  0.39886739, -1.18523127],\n",
       "       ...,\n",
       "       [ 5.8       ,  4.8       ,  1.4       ],\n",
       "       [ 5.8       ,  4.8       ,  1.6       ],\n",
       "       [ 5.8       ,  4.8       ,  1.8       ]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1d347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15072fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12b6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb83b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507bf9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
